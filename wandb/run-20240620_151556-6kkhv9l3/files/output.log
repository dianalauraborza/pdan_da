Epoch 0/99
----------
/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
Traceback (most recent call last):
  File "/home/areka/pdan_da/train_PDAN.py", line 374, in <module>
    run([(rgb_model, 0, dataloaders, optimizer, lr_sched, args.comp_info)], criterion, num_epochs=int(args.epoch))
  File "/home/areka/pdan_da/train_PDAN.py", line 146, in run
    train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)
  File "/home/areka/pdan_da/train_PDAN.py", line 220, in train_step
    outputs, loss, probs, err = run_network(model, data, gpu, epoch)
  File "/home/areka/pdan_da/train_PDAN.py", line 192, in run_network
    activation = model(inputs, mask_new)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 46, in forward
    out = self.stage1(x, mask, self.summary)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 66, in forward
    out = layer(out, mask, summary)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 79, in forward
    out = F.relu(self.conv_attention(x, summary))
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 130, in forward
    return self.forward_summary(x, summary)
  File "/home/areka/pdan_da/PDAN.py", line 149, in forward_summary
    padded_x, _ = self.cross_attention(query=padded_x, key=summary_expanded, value=summary_expanded)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py", line 5245, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py", line 4943, in _in_projection_packed
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
RuntimeError: CUDA out of memory. Tried to allocate 606.00 MiB (GPU 0; 14.58 GiB total capacity; 13.00 GiB already allocated; 340.75 MiB free; 13.20 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF