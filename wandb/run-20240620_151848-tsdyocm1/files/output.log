Epoch 0/99
----------
/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py:1944: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.
  warnings.warn("nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.")
/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
/home/areka/pdan_da/apmeter.py:108: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).
  rg = torch.range(1, self.scores.size(0)).float()
/home/areka/pdan_da/apmeter.py:136: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /opt/conda/conda-bld/pytorch_1646755888698/work/aten/src/ATen/native/IndexingUtils.h:28.)
  ap[k] = precision[truth.byte()].sum() / max(truth.sum(), 1)
train-map: tensor(9.9581)
val-map: tensor(14.8214)
tensor([22.8451, 19.9356, 22.4316, 13.1937, 15.3279, 11.0134, 17.2535,  1.8176,
        24.6746, 12.2441,  5.5686, 55.7182, 15.4454,  1.9739, 33.8267, 35.9135,
        37.5022,  1.5728,  4.4445, 10.5428, 28.1798, 17.1474,  6.4362,  6.3487,
         0.4853,  6.2705, 39.8246, 10.8628,  5.6438,  8.5503,  6.4458,  0.4098,
        36.2794, 21.2419,  7.2966,  7.4173,  2.7667, 14.7982, 14.7129,  3.4579,
        18.7361,  8.8554,  4.7602, 11.4438,  4.2399,  0.3256,  3.6403, 40.3607,
         3.3958,  6.3380,  2.7635, 44.2451, 46.6946, 10.9363,  7.5991, 11.3884,
         9.2168,  8.2860,  0.8378, 57.6426,  0.9028, 33.4978,  9.2255, 11.6397,
         0.6135, 13.6241,  1.7233, 14.5096,  1.3709,  1.7023, 25.9792, 10.1551,
        27.3120,  9.4591,  4.4390, 14.8726, 14.0381,  5.8404, 15.7450,  3.6323,
         2.0854,  9.1283, 10.3771,  1.2480,  5.0459,  0.6519,  1.0919,  6.6115,
         5.1564,  0.3401,  2.5004,  2.9465, 11.4736, 16.6582,  4.6629,  0.8399,
        18.6114, 29.8364, 51.1198,  2.5436,  8.5010,  0.1562, 53.4045,  1.1646,
         3.0257,  1.1876, 37.2846, 35.8957,  5.7303,  9.6428, 10.2414, 13.2085,
         7.0221, 13.3512, 12.0238, 21.3651,  3.6907,  3.9345, 27.8658,  9.9579,
         9.7932, 24.0254, 32.3644, 38.3793, 22.0684, 33.3134,  4.7577, 40.7923,
         5.2941,  2.0449,  9.2635,  0.9930, 29.5348, 13.1986, 31.3836, 34.2112,
         0.9996, 37.9382,  1.8835, 10.9914,  1.3582, 23.9972, 14.4789, 25.4054,
        17.2561, 28.5384, 16.8063, 49.2252, 26.7179, 16.2666, 26.9982, 12.8615,
        19.6705,  8.6575, 14.6203, 19.7473, 29.7325])
----> tensor(9.9581) 9.958088
Epoch 1/99
----------
Traceback (most recent call last):
  File "/home/areka/pdan_da/train_PDAN.py", line 374, in <module>
    run([(rgb_model, 0, dataloaders, optimizer, lr_sched, args.comp_info)], criterion, num_epochs=int(args.epoch))
  File "/home/areka/pdan_da/train_PDAN.py", line 146, in run
    train_map, train_loss = train_step(model, gpu, optimizer, dataloader['train'], epoch)
  File "/home/areka/pdan_da/train_PDAN.py", line 220, in train_step
    outputs, loss, probs, err = run_network(model, data, gpu, epoch)
  File "/home/areka/pdan_da/train_PDAN.py", line 192, in run_network
    activation = model(inputs, mask_new)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 46, in forward
    out = self.stage1(x, mask, self.summary)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 66, in forward
    out = layer(out, mask, summary)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 79, in forward
    out = F.relu(self.conv_attention(x, summary))
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/pdan_da/PDAN.py", line 130, in forward
    return self.forward_summary(x, summary)
  File "/home/areka/pdan_da/PDAN.py", line 149, in forward_summary
    padded_x, _ = self.cross_attention(query=padded_x, key=summary_expanded, value=summary_expanded)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1110, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/modules/activation.py", line 1038, in forward
    attn_output, attn_output_weights = F.multi_head_attention_forward(
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py", line 5245, in multi_head_attention_forward
    q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
  File "/home/areka/miniconda3/envs/pdan_a/lib/python3.10/site-packages/torch/nn/functional.py", line 4943, in _in_projection_packed
    return linear(q, w_q, b_q), linear(k, w_k, b_k), linear(v, w_v, b_v)
RuntimeError: CUDA out of memory. Tried to allocate 1.18 GiB (GPU 0; 31.74 GiB total capacity; 26.99 GiB already allocated; 1.00 GiB free; 29.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF